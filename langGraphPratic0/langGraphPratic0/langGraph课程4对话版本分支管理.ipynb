{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dec11ed5-16fe-4629-8c33-c89f14ebaf38",
   "metadata": {},
   "source": [
    "# æ—¶é—´æ—…è¡Œï¼ˆTime Travel\n",
    "LangGraph ä¸­çš„ **æ—¶é—´æ—…è¡Œï¼ˆTime Travelï¼‰** åŠŸèƒ½ï¼Œè¿™æ˜¯ä¸€ç§é«˜çº§çŠ¶æ€ç®¡ç†æœºåˆ¶ï¼Œå…è®¸ç”¨æˆ·å›žæº¯åˆ°å·¥ä½œæµçš„åŽ†å²çŠ¶æ€å¹¶é‡æ–°æ‰§è¡ŒåŽç»­æ­¥éª¤ã€‚ä»¥ä¸‹æ˜¯æ ¸å¿ƒæ¦‚å¿µçš„ä¸­æ–‡è§£æžï¼š\n",
    "\n",
    "---\n",
    "\n",
    "### â³ **æ—¶é—´æ—…è¡ŒåŠŸèƒ½çš„æ ¸å¿ƒä»·å€¼**\n",
    "1. **é—®é¢˜åœºæ™¯**ï¼š\n",
    "   - ä¼ ç»ŸèŠå¤©æœºå™¨äººæ˜¯**çº¿æ€§å¯¹è¯**ï¼šç”¨æˆ·è¾“å…¥ â†’ AI å“åº” â†’ ç»“æŸ\n",
    "   - **å®žé™…éœ€æ±‚**ï¼š\n",
    "     - ç”¨æˆ·æƒ³ä»Ž**åŽ†å²èŠ‚ç‚¹é‡æ–°æŽ¢ç´¢**ä¸åŒç»“æžœï¼ˆå¦‚ï¼š\"å›žåˆ°ç¬¬ä¸‰æ­¥å°è¯•å…¶ä»–æ–¹æ¡ˆ\"ï¼‰\n",
    "     - ä¿®å¤é”™è¯¯åŽ**é‡æ–°æ‰§è¡ŒåŽç»­æµç¨‹**ï¼ˆå¦‚ï¼šä¿®æ”¹å‚æ•°åŽé‡è¯•è®¡ç®—ï¼‰\n",
    "     - ç±»ä¼¼ Git çš„ç‰ˆæœ¬å›žé€€ï¼ˆå¸¸è§äºŽè‡ªä¸»ç¼–ç¨‹ Agentï¼‰\n",
    "\n",
    "2. **æŠ€æœ¯åŸºç¡€**ï¼š\n",
    "   - **æ£€æŸ¥ç‚¹ï¼ˆCheckpointsï¼‰**ï¼šLangGraph è‡ªåŠ¨ä¿å­˜æ¯ä¸ªæ­¥éª¤çš„å®Œæ•´çŠ¶æ€\n",
    "   - **æŒä¹…åŒ–å±‚**ï¼šæ‰€æœ‰åŽ†å²çŠ¶æ€å­˜å‚¨åœ¨æ•°æ®åº“ï¼ˆå¦‚ Redis/PostgreSQLï¼‰\n",
    "   - **çŠ¶æ€æ ‡è¯†**ï¼šæ¯ä¸ªçŠ¶æ€æœ‰å”¯ä¸€ IDï¼ˆå¦‚ `thread_id + step_id`ï¼‰\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”„ **æ—¶é—´æ—…è¡Œå·¥ä½œåŽŸç†**\n",
    "```mermaid\n",
    "sequenceDiagram\n",
    "    participant User\n",
    "    participant LangGraph\n",
    "    participant DB\n",
    "\n",
    "    User->>LangGraph: å‘èµ·ä»»åŠ¡\n",
    "    LangGraph->>DB: ä¿å­˜çŠ¶æ€(step1)\n",
    "    LangGraph->>User: å“åº”A\n",
    "    User->>LangGraph: ç»§ç»­ä»»åŠ¡\n",
    "    LangGraph->>DB: ä¿å­˜çŠ¶æ€(step2)\n",
    "    LangGraph->>User: å“åº”B\n",
    "\n",
    "    Note over User: ç”¨æˆ·é€‰æ‹©\"å›žåˆ°step1\"\n",
    "    User->>LangGraph: æäº¤åŽ†å²çŠ¶æ€ID\n",
    "    LangGraph->>DB: åŠ è½½step1çŠ¶æ€\n",
    "    LangGraph->>LangGraph: ä»Žstep1é‡æ–°æ‰§è¡Œ\n",
    "    LangGraph->>User: æ–°å“åº”A'\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ§© **å…³é”®å®žçŽ°è¦ç´ **\n",
    "1. **çŠ¶æ€åŠ è½½ API**ï¼š\n",
    "   ```python\n",
    "   # åŠ è½½åŽ†å²çŠ¶æ€\n",
    "   old_state = checkpoint.get(thread_id=\"123\", step_id=2)\n",
    "   \n",
    "   # ä»Žè¯¥çŠ¶æ€é‡æ–°æ‰§è¡Œ\n",
    "   new_state = graph.invoke(\n",
    "       input={\"new_input\": \"ä¿®æ­£åŽçš„å‚æ•°\"},\n",
    "       config={\"configurable\": {\"thread_id\": \"123\", \"step_id\": 2}}  # æŒ‡å®šèµ·ç‚¹\n",
    "   )\n",
    "   ```\n",
    "\n",
    "2. **åˆ†æ”¯æŽ¢ç´¢**ï¼š\n",
    "   - åŒä¸€åŽ†å²çŠ¶æ€å¯è¡ç”Ÿ**å¤šä¸ªæ‰§è¡Œåˆ†æ”¯**\n",
    "   - æ¯ä¸ªåˆ†æ”¯ç‹¬ç«‹å­˜å‚¨ï¼ˆç±»ä¼¼ä»£ç ä»“åº“çš„ Git branchï¼‰\n",
    "\n",
    "3. **çŠ¶æ€å®Œæ•´æ€§**ï¼š\n",
    "   - å›žé€€æ—¶è‡ªåŠ¨éš”ç¦»åŽŸå§‹çŠ¶æ€ï¼ˆé¿å…æ±¡æŸ“åŽ†å²æ•°æ®ï¼‰\n",
    "   - æ–°åˆ†æ”¯ç»§æ‰¿åŽŸçŠ¶æ€çš„**å·¥å…·è°ƒç”¨åŽ†å²/æ¶ˆæ¯è®°å¿†**\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸš€ **å…¸åž‹åº”ç”¨åœºæ™¯**\n",
    "1. **å†³ç­–å›žæº¯**  \n",
    "   - *åœºæ™¯*ï¼šé‡‘èžåˆ†æž Agent æŽ¨èè‚¡ç¥¨Aï¼Œç”¨æˆ·æƒ³éªŒè¯è‚¡ç¥¨B  \n",
    "   - *æ“ä½œ*ï¼šå›žé€€åˆ°åˆ†æžèŠ‚ç‚¹ï¼Œä¿®æ”¹è‚¡ç¥¨ä»£ç é‡æ–°æ‰§è¡Œ\n",
    "\n",
    "2. **é”™è¯¯ä¿®å¤**  \n",
    "   - *åœºæ™¯*ï¼šç¼–ç¨‹ Agent ç¬¬5æ­¥ä»£ç æŠ¥é”™  \n",
    "   - *æ“ä½œ*ï¼šå›žé€€åˆ°ç¬¬4æ­¥ï¼Œä¿®æ”¹ä»£ç åŽç»§ç»­æ‰§è¡Œ\n",
    "\n",
    "3. **ç­–ç•¥å¯¹æ¯”**  \n",
    "   - *åœºæ™¯*ï¼šå®žéªŒè®¾è®¡ Agent ç”Ÿæˆæ–¹æ¡ˆAï¼Œç”¨æˆ·æƒ³æ¯”è¾ƒæ–¹æ¡ˆB  \n",
    "   - *æ“ä½œ*ï¼šä»Žæ–¹æ¡ˆç”ŸæˆèŠ‚ç‚¹åˆ›å»ºä¸¤ä¸ªåˆ†æ”¯å¹¶è¡Œæ‰§è¡Œ\n",
    "\n",
    "4. **æ•™è‚²åœºæ™¯**  \n",
    "   - *åœºæ™¯*ï¼šæ•°å­¦è¾…å¯¼ Agent è§£é¢˜åˆ°ç¬¬3æ­¥ï¼Œå­¦ç”Ÿæƒ³å°è¯•ä¸åŒè§£æ³•  \n",
    "   - *æ“ä½œ*ï¼šå›žæº¯åˆ°ç¬¬2æ­¥é‡æ–°æŽ¨å¯¼\n",
    "\n",
    "---\n",
    "\n",
    "### âš ï¸ **å®žçŽ°æ³¨æ„äº‹é¡¹**\n",
    "1. **å·¥å…·å‰¯ä½œç”¨**ï¼š\n",
    "   - é¿å…é‡å¤æ‰§è¡Œæœ‰å‰¯ä½œç”¨çš„å·¥å…·ï¼ˆå¦‚æ”¯ä»˜/å†™æ•°æ®åº“ï¼‰\n",
    "   - æ–¹æ¡ˆï¼šæ ‡è®°å·¥å…·ä¸º `idempotent`ï¼ˆå¹‚ç­‰ï¼‰æˆ–æ·»åŠ å›žæ»šé€»è¾‘\n",
    "   ```python\n",
    "   @tool(idempotent=True)  # å£°æ˜Žå¹‚ç­‰å·¥å…·\n",
    "   def get_stock_price(symbol: str):\n",
    "       # å¯å®‰å…¨é‡å¤æ‰§è¡Œ\n",
    "   ```\n",
    "\n",
    "2. **çŠ¶æ€å­˜å‚¨ä¼˜åŒ–**ï¼š\n",
    "   - å¤§å°ºå¯¸çŠ¶æ€ï¼ˆå¦‚å›¾åƒç”Ÿæˆï¼‰éœ€åŽ‹ç¼©æˆ–å¤–éƒ¨å­˜å‚¨\n",
    "   - è®¾ç½®è‡ªåŠ¨æ¸…ç†ç­–ç•¥ï¼ˆä¿ç•™æœ€è¿‘ N ä¸ªç‰ˆæœ¬ï¼‰\n",
    "\n",
    "3. **åˆ†æ”¯ç®¡ç†**ï¼š\n",
    "   - ä¸ºåˆ†æ”¯æ·»åŠ è¯­ä¹‰æ ‡ç­¾ï¼ˆå¦‚ `branch: \"alternative_solution\"`ï¼‰\n",
    "   - å¯è§†åŒ–å·¥å…·å±•ç¤ºçŠ¶æ€æ ‘ï¼ˆç±»ä¼¼ Git Graphï¼‰\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ’¡ **æ€»ç»“**\n",
    "LangGraph çš„æ—¶é—´æ—…è¡ŒåŠŸèƒ½é€šè¿‡ï¼š\n",
    "1. **çŠ¶æ€å¿«ç…§**ï¼šè‡ªåŠ¨ä¿å­˜æ¯ä¸€æ­¥çš„å®Œæ•´ä¸Šä¸‹æ–‡  \n",
    "2. **ç²¾ç¡®å›žæº¯**ï¼šé€šè¿‡ `thread_id + step_id` å®šä½åŽ†å²çŠ¶æ€  \n",
    "3. **åˆ†æ”¯æ‰§è¡Œ**ï¼šä»Žä»»æ„èŠ‚ç‚¹è¡ç”Ÿæ–°è·¯å¾„  \n",
    "\n",
    "å®žçŽ°äº† **\"å¯é€†è®¡ç®—\"** èƒ½åŠ›ï¼Œè§£å†³äº† AI Agent å·¥ä½œæµçš„**å…³é”®ç—›ç‚¹**ï¼š\n",
    "- æŽ¢ç´¢ä¸ç¡®å®šæ€§æ—¶çš„è¯•é”™æˆæœ¬  \n",
    "- é”™è¯¯ä¼ å¯¼å¯¼è‡´çš„æŽ¨å€’é‡æ¥  \n",
    "- å¤šæ–¹æ¡ˆå¯¹æ¯”çš„é‡å¤æ‰§è¡Œ  \n",
    "\n",
    "> å¼€å‘è€…å»ºè®®ï¼šåœ¨æ¶‰åŠ**å¤æ‚å†³ç­–é“¾**ï¼ˆå¦‚ç§‘ç ”åˆ†æžã€ä»£ç ç”Ÿæˆï¼‰çš„åœºæ™¯ä¸­ä¼˜å…ˆé›†æˆæ­¤åŠŸèƒ½ï¼Œä¸ºç”¨æˆ·æä¾› **\"æ’¤é”€/é‡è¯•/åˆ†æ”¯æŽ¢ç´¢\"** çš„æ ¸å¿ƒäº¤äº’ä½“éªŒã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eed671b-8bf5-431f-8dee-6a4c61595b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "# è®¾ç½®è‡ªå®šä¹‰APIé…ç½®\n",
    "os.environ[\"QWEN_API_KEY\"] = \"You API Key\"\n",
    "os.environ[\"QWEN_API_BASE\"] = \"https://dashscope.aliyuncs.com/compatible-mode/v1\"\n",
    "\n",
    "# ä½¿ç”¨è‡ªå®šä¹‰é…ç½®\n",
    "llm = init_chat_model(\n",
    "    model=\"qwen-plus-latest\",\n",
    "    model_provider=\"openai\",\n",
    "    api_key=os.environ[\"QWEN_API_KEY\"],\n",
    "    base_url=os.environ[\"QWEN_API_BASE\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d2878c7-793e-4128-8258-be4907134d9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TAVILY_API_KEY:  Â·Â·Â·Â·Â·Â·Â·Â·\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import getpass\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "_set_env(\"TAVILY_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2649c908-3ef8-45fb-a6fb-26f08d0a8b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    "\n",
    "from langchain_tavily import TavilySearch\n",
    "from langchain_core.messages import ToolMessage\n",
    "from langchain_core.tools import InjectedToolCallId, tool\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "from langgraph.types import Command, interrupt\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "    name: str\n",
    "    birthday: str\n",
    "\n",
    "@tool\n",
    "def human_assistance(\n",
    "    name: str, birthday: str, tool_call_id: Annotated[str, InjectedToolCallId]\n",
    ") -> str:\n",
    "    \"\"\"Request assistance from a human.\"\"\"\n",
    "    human_response = interrupt(\n",
    "        {\n",
    "            \"question\": \"Is this correct?\",\n",
    "            \"name\": name,\n",
    "            \"birthday\": birthday,\n",
    "        },\n",
    "    )\n",
    "    if human_response.get(\"correct\", \"\").lower().startswith(\"y\"):\n",
    "        verified_name = name\n",
    "        verified_birthday = birthday\n",
    "        response = \"Correct\"\n",
    "    else:\n",
    "        verified_name = human_response.get(\"name\", name)\n",
    "        verified_birthday = human_response.get(\"birthday\", birthday)\n",
    "        response = f\"Made a correction: {human_response}\"\n",
    "\n",
    "    state_update = {\n",
    "        \"name\": verified_name,\n",
    "        \"birthday\": verified_birthday,\n",
    "        \"messages\": [ToolMessage(response, tool_call_id=tool_call_id)],\n",
    "    }\n",
    "    return Command(update=state_update)\n",
    "\n",
    "\n",
    "tool = TavilySearch(max_results=2)\n",
    "tools = [tool, human_assistance]\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "def chatbot(state: State):\n",
    "    message = llm_with_tools.invoke(state[\"messages\"])\n",
    "    assert(len(message.tool_calls) <= 1)\n",
    "    return {\"messages\": [message]}\n",
    "\n",
    "graph_builder = StateGraph(State)\n",
    "graph_builder.add_node(\"chatbot\", chatbot)\n",
    "\n",
    "tool_node = ToolNode(tools=tools)\n",
    "graph_builder.add_node(\"tools\", tool_node)\n",
    "\n",
    "graph_builder.add_conditional_edges(\n",
    "    \"chatbot\",\n",
    "    tools_condition,\n",
    ")\n",
    "graph_builder.add_edge(\"tools\", \"chatbot\")\n",
    "graph_builder.add_edge(START, \"chatbot\")\n",
    "\n",
    "memory = MemorySaver()\n",
    "graph = graph_builder.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ccc9bb7a-9cd7-4113-9bd0-191c4f355512",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "I'm learning LangGraph. Could you do some research on it for me?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "I can help you with that. Could you provide me with more details on what specific aspects of LangGraph you're interested in? For example, are you looking for an overview of its features, tutorials on how to use it, or something else?\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "events = graph.stream(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": (\n",
    "                    \"I'm learning LangGraph. \"\n",
    "                    \"Could you do some research on it for me?\"\n",
    "                ),\n",
    "            },\n",
    "        ],\n",
    "    },\n",
    "    config,\n",
    "    stream_mode=\"values\",\n",
    ")\n",
    "for event in events:\n",
    "    if \"messages\" in event:\n",
    "        event[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af4ba39b-4f35-4c2c-a3cb-e8675789699e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Ya that's helpful. Maybe I'll build an autonomous agent with it!\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "That sounds like a great plan! LangGraph is well-suited for building agents as it allows you to define complex workflows with ease. Here are some key points and steps that can help guide your project of building an autonomous agent:\n",
      "\n",
      "### **Key Concepts in LangGraph**\n",
      "1. **Graph-Based Workflows**: LangGraph lets you model workflows as graphs, making it easier to visualize and manage the flow of data and execution.\n",
      "2. **Modular Components**: You can create modular components (nodes) representing specific tasks or functions, which can be reused across different workflows.\n",
      "3. **State Management**: It provides tools to manage state effectively, allowing agents to remember previous interactions and make decisions based on historical data.\n",
      "4. **Integration with LLMs**: LangGraph integrates seamlessly with large language models (LLMs), enabling you to build intelligent agents capable of reasoning and decision-making.\n",
      "\n",
      "### **Steps to Build an Autonomous Agent Using LangGraph**\n",
      "1. **Define the Goal**:\n",
      "   - Clearly outline what your agent should achieve. For example, is it a customer service chatbot, a personal assistant, or something else?\n",
      "\n",
      "2. **Design the Workflow**:\n",
      "   - Use LangGraph's graph-based approach to design how your agent will process inputs, make decisions, and generate outputs. Identify nodes for each task (e.g., input parsing, decision-making, response generation).\n",
      "\n",
      "3. **Implement Nodes**:\n",
      "   - Develop individual components (nodes) for each step in the workflow. These could include prompt templates, LLM calls, external API integrations, or custom logic.\n",
      "\n",
      "4. **Integrate State Management**:\n",
      "   - Implement mechanisms for your agent to store and retrieve context from past interactions. This helps in providing personalized and coherent responses.\n",
      "\n",
      "5. **Test and Iterate**:\n",
      "   - Test your agent thoroughly to ensure it behaves as expected under various scenarios. Refine the workflow and node implementations based on feedback.\n",
      "\n",
      "6. **Deploy and Monitor**:\n",
      "   - Deploy your agent to the desired environment and monitor its performance. Make improvements based on real-world usage and user feedback.\n",
      "\n",
      "If you're looking for tutorials or examples to get started, I can guide you to resources or provide sample code snippets tailored to your use case. Let me know if you'd like to dive deeper into any of these areas!\n"
     ]
    }
   ],
   "source": [
    "events = graph.stream(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": (\n",
    "                    \"Ya that's helpful. Maybe I'll \"\n",
    "                    \"build an autonomous agent with it!\"\n",
    "                ),\n",
    "            },\n",
    "        ],\n",
    "    },\n",
    "    config,\n",
    "    stream_mode=\"values\",\n",
    ")\n",
    "for event in events:\n",
    "    if \"messages\" in event:\n",
    "        event[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a9d914d7-dc4d-4f80-950f-5713d6dcba68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Messages:  8 Next:  ()\n",
      "--------------------------------------------------------------------------------\n",
      "Num Messages:  7 Next:  ('chatbot',)\n",
      "--------------------------------------------------------------------------------\n",
      "Num Messages:  6 Next:  ('__start__',)\n",
      "--------------------------------------------------------------------------------\n",
      "Num Messages:  6 Next:  ()\n",
      "--------------------------------------------------------------------------------\n",
      "Num Messages:  5 Next:  ('chatbot',)\n",
      "--------------------------------------------------------------------------------\n",
      "Num Messages:  4 Next:  ('__start__',)\n",
      "--------------------------------------------------------------------------------\n",
      "Num Messages:  4 Next:  ()\n",
      "--------------------------------------------------------------------------------\n",
      "Num Messages:  3 Next:  ('chatbot',)\n",
      "--------------------------------------------------------------------------------\n",
      "Num Messages:  2 Next:  ('tools',)\n",
      "--------------------------------------------------------------------------------\n",
      "Num Messages:  1 Next:  ('chatbot',)\n",
      "--------------------------------------------------------------------------------\n",
      "Num Messages:  0 Next:  ('__start__',)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "to_replay = None\n",
    "for state in graph.get_state_history(config):\n",
    "    print(\"Num Messages: \", len(state.values[\"messages\"]), \"Next: \", state.next)\n",
    "    print(\"-\" * 80)\n",
    "    if len(state.values[\"messages\"]) == 6:\n",
    "        # We are somewhat arbitrarily selecting a specific state based on the number of chat messages in the state.\n",
    "        to_replay = state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "45ac13b9-966f-4b77-b3c2-8294147c684d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "()\n",
      "{'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f061241-0473-6e16-8006-fd937852f970'}}\n"
     ]
    }
   ],
   "source": [
    "print(to_replay.next)\n",
    "print(to_replay.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8c10458b-4b0c-4d39-94c7-c2b81f9bf0a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "I can help you with that. Could you provide me with more details on what specific aspects of LangGraph you're interested in? For example, are you looking for an overview of its features, tutorials on how to use it, or something else?\n"
     ]
    }
   ],
   "source": [
    "# The `checkpoint_id` in the `to_replay.config` corresponds to a state we've persisted to our checkpointer.\n",
    "for event in graph.stream(None, to_replay.config, stream_mode=\"values\"):\n",
    "    if \"messages\" in event:\n",
    "        event[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2383a924-4f80-4620-8e18-8eb4a6a29efc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
